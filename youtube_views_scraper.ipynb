{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL - Scraping YouTube Video Playlist Stats with Python\n",
    "\n",
    "One of the most challenging aspects of working in social media marketing for entertainment is getting an accurate portrayal of reach when a new video is released as part of a major beat.\n",
    "\n",
    "A big part of your reach will come from owned channels, while others may come from media parters, and even superfans who rip video content for their own channels. Believe me when I say I've spent a large deal of time between YouTube and a manual spreadsheet trying to record the internet's reception of our campaign's content after the first day, second day, third day and so on...\n",
    "\n",
    "In this project, I'll walk you through a webscraping tool I developed to automate YouTube statistics recording for loading into a dataframe or database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##browser controller\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "#webscraping and string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "#dataframe management\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Cleaning Functions\n",
    "\n",
    "Before we fire up the browser controller, it's important to first walk through a few of these string cleaning functions that are unique to the YouTube platform.\n",
    "\n",
    "\n",
    "![](https://github.com/gtieng/youtube_trailer_scraper/blob/master/markdown_images/yt_km.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection of the metadata we want to scrape, you'll see that subscribers, views, and likes/dislikes will result in strings with commas, Ks and Ms (for thousands and millions, respectively) that will all need to be cleaned and cast into a numeric type. The following code block is a function to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_cleaner(value):\n",
    "    #removes subscribers or views from string\n",
    "    value = re.sub(' subscribers', '', value)\n",
    "    value = re.sub(' views', '', value)\n",
    "    \n",
    "    #casts string to thousand or million numeric based on K or M value\n",
    "    if 'K' in value:\n",
    "        value = float(re.sub('K', '', value))\n",
    "        value *= 1000\n",
    "        return int(value)\n",
    "    elif 'M' in value:\n",
    "        value = float(re.sub('M', '', value))\n",
    "        value *= 1000000\n",
    "        return int(value)\n",
    "    else:\n",
    "        return int(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other string to format for this function is the date. In the occurence that the uploaded video is less than a day old, YouTube may display the date in hours from its premiere date as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/gtieng/youtube_trailer_scraper/blob/master/markdown_images/yt_ago.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_uploaded(value):\n",
    "    value = re.sub('â€¢', '', value)\n",
    "    if 'ago' in value:\n",
    "        value = int(re.search(r'\\d+', value).group())\n",
    "        uploaded = (datetime.now() - timedelta(hours=value))\n",
    "        return datetime.strftime(uploaded, '%b %-d, %Y')\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Video Pages\n",
    "\n",
    "With the string cleaning functions complete, we are now able to scrape single YouTube video pages to collect its stats. Using BeautifulSoup, we'll be able to extract the following information for our dataframe:\n",
    "\n",
    "- Video Title\n",
    "- Video URL\n",
    "- Account Name\n",
    "- Subscribers\n",
    "- Video Views\n",
    "- Date Uploaded\n",
    "- Number of Likes\n",
    "- Numbr of Dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(youtube_url):\n",
    "    \n",
    "    #browser controller\n",
    "    driver = webdriver.Chrome('resources/chromedriver')\n",
    "    driver.get(youtube_url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    driver.quit()\n",
    "    \n",
    "    #values that need no formatting\n",
    "    title_value = soup.find('h1').text\n",
    "    account_value = soup.find('yt-formatted-string', class_='ytd-channel-name').text\n",
    "    \n",
    "    #values that need date_uploaded function\n",
    "    date_value = date_uploaded(soup.find('div', id='date').text)\n",
    "    \n",
    "    #values that need km_cleaner function\n",
    "    subs_value = km_cleaner(soup.find('yt-formatted-string', id='owner-sub-count').text)\n",
    "    view_count = km_cleaner(soup.find('span', class_='short-view-count').text)\n",
    "    likes_value = km_cleaner(soup.find_all('yt-formatted-string', class_='ytd-toggle-button-renderer')[0].text)\n",
    "    dislikes_value = km_cleaner(soup.find_all('yt-formatted-string', class_='ytd-toggle-button-renderer')[1].text)\n",
    "    \n",
    "    #combined dictionary of values\n",
    "    stats = {'date_uploaded': date_value,\n",
    "             'account': account_value,\n",
    "             'video_title': title_value,\n",
    "             'views': view_count,\n",
    "             'subscribers': subs_value,\n",
    "             'likes': likes_value,\n",
    "             'dislikes': dislikes_value,\n",
    "             'url': youtube_url\n",
    "            }\n",
    "    \n",
    "    return stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_uploaded': 'Jan 24, 2016',\n",
       " 'account': 'Games Done Quick',\n",
       " 'video_title': 'The Simpsons Arcade Game by Maquina_azul30 in 18:49 - Awesome Games Done Quick 2016 - Part 46',\n",
       " 'views': 108000,\n",
       " 'subscribers': 826000,\n",
       " 'likes': 824,\n",
       " 'dislikes': 37,\n",
       " 'url': 'https://www.youtube.com/watch?v=2MIgvCHSsHY'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats('https://www.youtube.com/watch?v=2MIgvCHSsHY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Playlist Pages\n",
    "\n",
    "Now that we have the ability to scrape one page based on the provided URL, we'll now write a scraping function to create a list of extracted video urls from a YouTube playlist page. The hardest challenge of this step is navigating the infinite scroll to load the full list of videos when there is more than 100 videos in the playlist. This problem was solved by programming the scroller to go to the bottom of the page for every 100 videos listed in the vidieo count summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_urls(youtube_playlist_url):\n",
    "    #webdriver setup & html scrape\n",
    "    driver = webdriver.Chrome('resources/chromedriver')\n",
    "    driver.get(youtube_playlist_url)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    #isolate the video stats\n",
    "    video_stats = soup.find_all('yt-formatted-string')[5].text\n",
    "    #use regex to isolate video number\n",
    "    no_of_videos = re.sub(',', '', video_stats)\n",
    "    no_of_videos = re.sub(' videos', '', video_stats)\n",
    "    #number of scroll based on 100 thumbnails per scroll\n",
    "    no_of_scrolls = int(int(no_of_videos) / 100 + 1)\n",
    "\n",
    "    #scroll page for every 100 videos\n",
    "    for i in range(no_of_scrolls):\n",
    "        driver.execute_script(\"window.scrollBy(0, 12000);\")\n",
    "        time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    driver.quit()\n",
    "\n",
    "    #extract youtube playlist urls\n",
    "    matches = soup.find_all('a', class_='yt-simple-endpoint style-scope ytd-playlist-video-renderer')\n",
    "    youtube_urls = []\n",
    "    for url in matches:\n",
    "        string = url.get('href')\n",
    "        substring = re.search('\\/watch\\?v=([^&]+)', string).group()\n",
    "        youtube_urls.append('http://www.youtube.com' + substring)    \n",
    "    return youtube_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, we'll chain the two scraping functions into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_export(url):\n",
    "    all_urls = get_video_urls(url)\n",
    "    all_stats = []\n",
    "    for video in all_urls:\n",
    "        all_stats.append(get_stats(video))\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwe = create_export('https://www.youtube.com/playlist?list=PL7qtZGedQPadDiw6Y-XgiQIk035CQc3pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"views\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "**Gerard Tieng** â€” *Data Analyst & Social Media Marketer*\n",
    "- [http://www.twitter.com/gerardtieng](http://www.twitter.com/gerardtieng)\n",
    "- [http://www.linkedin.com/in/gerardtieng](http://www.linkedin.com/in/gerardtieng)\n",
    "- [http://www.github.com/gtieng](http://www.github.com/gtieng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
